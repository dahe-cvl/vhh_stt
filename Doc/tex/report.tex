\documentclass[11pt]{article}
\usepackage[english]{babel}
\usepackage{lmodern} % better font
\usepackage[medium]{titlesec} % control headline sizes
\usepackage[utf8]{inputenc}
\usepackage[margin=3cm]{geometry}
\usepackage[activate={true,nocompatibility},final,babel=true,tracking=true]{microtype} % sexy micro typesetting
\usepackage{amssymb}
\usepackage{amsmath} % align environment etc.
\usepackage{hyperref}
\usepackage[table]{xcolor}% http://ctan.org/pkg/xcolor
% \usepackage{csvsimple} % load table data from csv
\usepackage{filecontents, catchfile}
\usepackage{booktabs} % table toprule etc.
\usepackage{ragged2e} % >{\Centering} in table
\usepackage{makecell} % multi line cell
\usepackage{graphicx} % \includegraphics
\usepackage{commath} % \abs etc.
\usepackage{siunitx}
\usepackage{todonotes}
\usepackage[backend=biber,style=numeric]{biblatex} %biblatex with biber
\addbibresource{report.bib} % bibliography style

\newcommand{\cbox}[3][black]{\textcolor[rgb]{#1}{\rule{#2}{#3}}}
\newcommand{\cellcomment}[3][tl]{\makecell[#1]{#2 \\[-1.5mm] {\scriptsize #3}}}
\mathchardef\mhyphen="2D % Define a "math hyphen"

\titleformat{\subsection}
  {\normalfont\bfseries}{\thesubsection}{1em}{}
\setlength{\parindent}{0pt}

\begin{document}

\huge{Evaluation of Audio Transcription Services}
\vspace{1mm}
\hrule
\vspace{2mm}
\large{WP6 - \textit{Automated media analysis}}
\vspace{5mm}

\section{Introduction}

Nowadays, a range of automatic audio transcription services are offered by providers such as Google or Amazon.
Most of these services are cloud-based and support transcription for a fixed set of languages, with the exception of CMU Sphinx -- a language independent transcription framework that can be used with any language for which a language model can be provided.
In contrast cloud-based services work out of the box and allow the user to offload non-negligible computational loads to an external device.
We evaluate the suitability of four transcription services on a small dataset spanning $13$ languages. 

\section{Dataset}

The key concept audio handles the automated transcription of $14$ interviews covering $13$ languages.
These interviews are available in either audio or video form.
Two samples contain Russian language, the first one being purely russian (labeled \textit{Russian(1)} in Figures \ref{fig:word_error_rate}, \ref{fig:tf_idf} and \ref{fig:term_frequency}) and the other consisting of a Russian/Ukrainian mix (labeled \textit{Russian(2)}).
The second is considered to be Russian since transcription tests show considerably worse results if the origin language is set to Ukrainian.
The full set of evaluated languages is as follows: English, Spanish, French, Italian, Russian, Dutch, Hebrew, Greek, Polish, Hungarian, Serbian, Slovenian and Czech.
Interviews are between $1.5$ and $5$ hours long and are mostly audio encoded in the \texttt{mp3} format.
Four samples (Italian, Hungarian, Serbian and Polish) are video that is encoded in the \texttt{mpg} format.
In order to minimize the risk that personal information of interviewees is exposed to cloud-based services the first and last $15$ minutes of each interview are discarded during preprocessing.
Ground truth data is generated by professional human transcriptionists and is available in the \texttt{pdf} format.
It is converted into plain text to facilitate further automated processing. 
This conversion step may introduce a small number of artificial errors since inconsistently formatted additional information such as timestamps and comments need to be removed.

\section{Service Overview}

This evaluation tests the suitability of the four services \emph{Google Cloud Speech-to-Text}\footnote{https://cloud.google.com/speech-to-text/}, \emph{Azure Speech-to-Text}\footnote{https://azure.microsoft.com/en-us/services/cognitive-services/speech-to-text/}, \emph{Amazon Transcribe}\footnote{https://aws.amazon.com/de/transcribe/} and \emph{CMU Sphinx}\footnote{https://cmusphinx.github.io/}.
All transcriptions are obtained through the respective Python APIs.
Utility, ease of use and to a lesser extent transcription speed may differ if APIs for other programming languages are used instead.

\paragraph{Google Cloud Speech-to-Text:}
With a total of $120$ languages and variants, and support for all of the languages considered here, Google Cloud Speech-to-Text is the best option in terms of availability. 
It further distinguishes itself with an easy to use API and fast transcription.
By default data logging is disabled and its cost is set to $0.024$ \$/min .
The data logging option allows Google to record audio data sent to Cloud Speech-to-Text and reduces the cost to $0.016$ \$/min.
For audio clips longer than one minute in length the API only allows asynchronous requests which require the file to be uploaded to Google Cloud Storage.
Shorter audio clips can be transcribed through a synchronous request without explicit use of cloud storage.
This service specifically discourages the use of lossy audio compression as used in the \texttt{mp3} format.

\paragraph{Azure Speech-to-Text:}
The transcription service offered by Microsoft Azure supports $39$ languages and variants in total and $7$ of the evaluated languages. 
Its cost is set to $0.0167$ \$/min.
Both synchronous and asynchronous transcriptions are available for audio clips of arbitrary length.
The documentation does not specify whether data is kept after the transcription process has finished or if it is used for training purposes.
While transcription is slightly slower, this service allows the user to run up to $20$ transcription jobs in parallel.

\paragraph{Amazon Transcribe:}
Amazon Transcribe offers $31$ languages and variants with support for $7$ of the evaluated languages at a cost of $0.024$ \$/min.
This service is tightly integrated into the AWS ecosystem, a transcription job can only be started on data that has been uploaded to the Amazon S3 cloud storage service.
Although transcription speed compared to other services is slow, Amazon Transcribe allows the user to start up to 100 concurrent transcription jobs.
Any voice inputs processed by the service may be stored and used by AWS for the training of speech recognition models.

\paragraph{CMU Sphinx:}
The offline software CMU Sphinx is in principle language independent, it can transcribe any language for which a language model is available. 
Models for $15$ languages ($7$ in the evaluated language set) have been collected by the developers and can be downloaded for free\footnote{https://sourceforge.net/projects/cmusphinx/files/Acoustic\%20and\%20Language\%20Models/}. 
Transcription speed varies to a great extend between languages. 
The number of parallel transcription jobs is limited by the available computing power. \\

Other services such as \emph{AssemblyAI} and \emph{IBM Watson} have been initially considered but disregarded due to lacking language support. 
An overview of costs, transcription speed and summarized language support is given in Table \ref{tbl:services} while Table \ref{tbl:availability} shows language availability in detail.

\begin{table}[!ht]
	\centering
	\begin{tabular}{llcl}
		\toprule
		Service & Cost & Languages & Transcription Speed \\
		\midrule
		Google Cloud Speech-to-Text & $0.0240$ \small \$/min	& \makebox[8mm][r]{13/13} 	& \cellcomment[tl]{$5.0$ \small min/min}{up to $1$ parallel job} \\
		Azure Speech-to-Text 		& $0.0167$ \small \$/min	& \makebox[8mm][r]{7/13} 	& \cellcomment[tl]{$2.0$ \small min/min}{up to $20$ parallel jobs} \\
		Amazon Transcribe 			& $0.0240$ \small \$/min	& \makebox[8mm][r]{7/13} 	& \cellcomment[tl]{$0.5$ \small min/min}{up to $100$ parallel jobs} \\
		CMU Sphinx					& free (offline)			& \makebox[8mm][r]{7/13}	& \cellcomment[tl]{$0.4-4.2$ \small min/min}{measured on local device} \\
		\bottomrule
	\end{tabular}
	\caption{Overview of cost, language support and transcription speed (ratio of audio duration to transcription duration) of the compared services.}
  \label{tbl:services}
\end{table}

\CatchFileDef{\availabilityTbl}{../visuals/availability_table.tex}{}
\begin{table}[!ht]
	\centering
	\begin{tabular}{l>{\Centering}m{20mm}>{\Centering}m{20mm}>{\Centering}m{20mm}>{\Centering}m{20mm}}
		\toprule
		Language & Google Cloud STT & Azure STT & Amazon Transcribe & CMU Sphinx \\
		\midrule
		\availabilityTbl
		\bottomrule
	\end{tabular}
	\caption{Detailed language availability for each service.}
  \label{tbl:availability}
\end{table}

\section{Metrics}

The evaluation of transcription quality is a non-trivial matter. 
Metrics such as Word Error Rate can only be computed if two given word sequences are not too dissimilar, while tf-idf fails whenever a word sequence cannot be separated into discrete sentences.
Some metrics measure similarity on a word-by-word basis while others make statistical assertions based on the respective vocabularies.
Metrics corresponding to both approaches have been used to ensure that at least one measurement can be made for each transcribed document and transcription service.

\subsection*{Word Error Rate}

The standard measure\,\cite{WER1, WER2, WER3} in speech recognition evaluation is \emph{Word Error Rate}, or WER for short.
In general it is not guaranteed that the length of a recognized word sequence is identical to its correct reference sequence.
For this reason it is necessary to perform a string alignment step that attempts to line up identical words occurring in both sequences.
Any word for which no counterpart is found falls into one of three categories:
\begin{itemize}
	\itemsep 0em
	\item \textbf{Substitution:} A word that has been recognized in place of a different word in the reference sequence.
	\item \textbf{Deletion:} A word that occurs in the reference sequence but is missing in the recognized sequence.
	\item \textbf{Insertion:} A word that occurs in the recognized sequences but does not exist in the reference sequence.
\end{itemize}
The Word Error Rate is then defined as:
\begin{align}
	\mathrm{WER} = \frac{S + D + I}{N}
\end{align}
where $S$ is the number of substitutions, $D$ is the number of deletions, $I$ is the number of insertions and $N$ is the total number of words in the reference sequence.
In general a lower Word Error Rate indicates better transcription accuracy.
The string alignment step may fail if the recognized word sequence diverges too far from the reference sequence.
Other metrics are considered in order to retain comparability in such cases, however we consider WER to be the most meaningful metric.

\subsection*{tf-idf and Term Frequency}

The tf-idf metric is a statistical quantity that indicates how important a word is to a single document in a collection of documents.
It is frequently used for document classification and the evaluation of keyword detection algorithms\,\cite{TFIDF1, TFIDF2}.
The name tf-idf is composed of abbreviations for the terms Term Frequency (tf) and Inverse Document Frequency (idf). 
If the number of occurrences of the term $t$ in the document $D$ is denoted $\#(t, D)$ then the Term Frequency is defined as
\begin{align}
	\mathrm{tf}(t, D) = \frac{\#(t, D)}{\max_{t' \in D} \#(t', D)}.
\end{align}
It is the normalized frequency of $t$ in $D$.
The main advantage of this metric is that it can be computed for any two non-empty word sequences. \\

The inverse document frequency measures the amount of information obtained by the occurrence of a word in a document in relation to the entire corpus.
For a term $t$ and a corpus $\mathcal{C}$ it is defined as
\begin{align}
	\mathrm{idf}(t, \mathcal{C}) = -\log \frac{\abs{\{D \in \mathcal{C} : t \in D\}}}{\abs{\mathcal{C}}}.
\end{align}
The combination of Term Frequency and Inverse Document Frequency yields tf-idf: 
\begin{align}
	\mathrm{tf\mhyphen idf}(t, D, \mathcal{C}) = \mathrm{tf}(t, D) \mathrm{idf}(t, \mathcal{C}).
\end{align}
For the purposes of this evaluation the term ``document'' refers to a single sentence in a word sequence and ``corpus'' refers to the entire word sequence.
In direct consequence a tf-idf score can only be computed if the transcription service provides punctuation symbols. \\

Both tf-idf and Term Frequency are transformations that allow the conversion of word sequences into real valued vectors. 
After enumerating the total set of words that occur in two (or more) word sequences, the $i$-th entry of such a vector is calculated as the tf-idf score (or Term Frequency) of the $i$-th word.
The similarity score between two vectors $\mathbf{a}, \mathbf{b}$ is then calculated as their cosine similarity:
\begin{align}
	\mathrm{sim}(\mathbf{a}, \mathbf{b}) = \frac{\mathbf{a} \cdot \mathbf{b}}{\norm{\mathbf{a}} \norm{\mathbf{b}}}.
\end{align}

\section{Evaluation}

The Word Error Rate metric is considered to be the default measure in automated speech recognition. 
In contrast to Term Frequency and tf-idf it is based on transcription success on a word-by-word basis.
We consider transcription service rankings based on this metric to be the most meaningful.
Evaluation results based on WER are shown in Figure \ref{fig:word_error_rate} and Table \ref{tbl:word_error_rate}.
Excluding two languages, English and French, the service Google Cloud Speech-to-Text delivers the best results while scores for the two exceptions are still competitive. \\
\begin{figure}[!ht]
	\centering
	\includegraphics[width=1.0\textwidth]{../visuals/word_error_rate.pdf}
	\vspace{-7mm}
	\caption{The word error rate achieved by services across all languages (lower is better).}
	\label{fig:word_error_rate}
\end{figure}
\CatchFileDef{\werTbl}{../visuals/wer_table.tex}{}
\begin{table}[!ht]
	\centering
	\begin{tabular}{l>{\Centering}m{20mm}>{\Centering}m{20mm}>{\Centering}m{20mm}>{\Centering}m{20mm}}
		\toprule
		Language & Google Cloud STT & Azure STT & Amazon Transcribe & CMU Sphinx \\
		\midrule
		\werTbl
		\bottomrule
	\end{tabular}
	\caption{The word error rate achieved by services across all languages (lower is better).}
  \label{tbl:word_error_rate}
\end{table}

In contrast to WER, the metrics Term Frequency and tf-idf merely measure the similarity of term distributions between two word sequences. 
Such a score would remain unchanged if, for example, the words in all sentences of a word sequence were reversed.
Results based on tf-idf are shown in Figure \ref{fig:tf_idf} and Table \ref{tbl:tf_idf} while results based on Term Frequency are shown in Figure \ref{fig:term_frequency} and Table \ref{tbl:term_frequency}.
For tf-idf the ranking between services is less clear, with Google Cloud Speech-to-Text, Azure Speech-to-Text and Amazon Transcribe scoring generally similar and each having the highest score for more than one language.
For some service/language pairs this metric could not be computed due to missing separation of the transcribed text into individual sentences as this feature was not always offered.
\begin{figure}[!ht]
	\centering
	\includegraphics[width=1.0\textwidth]{../visuals/tf_idf.pdf}
	\vspace{-7mm}
	\caption{The tf-idf scores achieved by services across all languages (higher is better).}
	\label{fig:tf_idf}
\end{figure}
\CatchFileDef{\tfidfTbl}{../visuals/tf_idf_table.tex}{}
\begin{table}[!ht]
	\centering
	\begin{tabular}{l>{\Centering}m{20mm}>{\Centering}m{20mm}>{\Centering}m{20mm}>{\Centering}m{20mm}}
		\toprule
		Language & Google Cloud STT & Azure STT & Amazon Transcribe & CMU Sphinx \\
		\midrule
		\tfidfTbl
		\bottomrule
	\end{tabular}
	\caption{The tf-idf scores achieved by services across all languages (higher is better).}
  \label{tbl:tf_idf}
\end{table}

The measurements in Figure \ref{fig:term_frequency} are more spread out with Azure Speech-to-Text and Amazon Transcribe mostly scoring the highest, however common words such as ``a'' or ``the'' are assigned a disproportionately high term frequency score despite their low importance to a documents content.
For this reason a score calculated solely based on Term Frequencies has limited informative value.
\begin{figure}[!ht]
	\centering
	\includegraphics[width=1.0\textwidth]{../visuals/term_frequency.pdf}
	\vspace{-7mm}
	\caption{The Term Frequency scores achieved by services across all languages (higher is better).}
	\label{fig:term_frequency}
\end{figure}
\CatchFileDef{\tfTbl}{../visuals/tf_table.tex}{}
\begin{table}[!ht]
	\centering
	\begin{tabular}{l>{\Centering}m{20mm}>{\Centering}m{20mm}>{\Centering}m{20mm}>{\Centering}m{20mm}}
		\toprule
		Language & Google Cloud STT & Azure STT & Amazon Transcribe & CMU Sphinx \\
		\midrule
		\tfTbl
		\bottomrule
	\end{tabular}
	\caption{The Term Frequency scores achieved by services across all languages (higher is better).}
  \label{tbl:term_frequency}
\end{table}

\section{Conclusion}

We compared the suitability of four automated transcription services on a dataset containing $13$ languages.
Appropriate metrics have been identified and it has been determined that, if applicable, the Word Error Rate measure give the best reflection of transcription quality.
Based on quantitative results using this metric as well as due to its superior language support compared to other services in the lineup we determined that Google Cloud Speech-to-Text is best suited for the automated transcription of audio data related to the VHH project.

% Literatur
\clearpage
\printbibliography

\end{document}

